{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "412d3c6b",
   "metadata": {},
   "source": [
    "# IMDB to Hotel Sentiment\n",
    "\n",
    "In this notebook we will take a sentiment analysis model trained on IMDB reviews, and fine tune it to analyse tweets about hotels. We will use AdaTest to help us generate a suitable test suite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9f4de2",
   "metadata": {},
   "source": [
    "## Seeding the PRNG\n",
    "\n",
    "Before we do anything else, we first seed the PRNG, to ensure that we have reproducible results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c907dfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(1012351)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52054118",
   "metadata": {},
   "source": [
    "## The Base Model\n",
    "\n",
    "We will use the [`aychang/roberta-base-imdb` from Hugging Face](https://huggingface.co/aychang/roberta-base-imdb) as our base model. This is a binary model which has been trained on a collection of IMDB reviews. First, we load the model itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1c4c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "base_model_name = \"aychang/roberta-base-imdb\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(base_model_name,num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "original_pipeline = pipeline(\"sentiment-analysis\",\n",
    "                             model=model,\n",
    "                             tokenizer=tokenizer,\n",
    "                             top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e5e15b",
   "metadata": {},
   "source": [
    "Now, let's try a few sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8960e590",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_pipeline(\"Great cinematography but a poor movie overall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb116e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_pipeline(\"Snappy dialogue makes for enjoyable entertainment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619e6c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_pipeline(\"Located on a busy street with much traffic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83c2a06",
   "metadata": {},
   "source": [
    "We can see that the two statements about movies are well classified, but the one about the final one about the hotel is not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d90d0d8",
   "metadata": {},
   "source": [
    "## Using AdaTest\n",
    "\n",
    "AdaTest is a tool to help create training/test suites for language models. The basic workflow is:\n",
    "\n",
    "1. User provides some sample input\n",
    "1. User flags whether the model output is correct or not\n",
    "1. AdaTest uses a second language model to general more inputs from those already provided\n",
    "1. User decides which of the AdaTest proposed inputs to incorporate (and whether the model provided a correct response)\n",
    "\n",
    "Iterating through this process a few times can generate a lot of tests quite quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48bbbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import adatest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139763f8",
   "metadata": {},
   "source": [
    "For our generator, we use OpenAI's GPT-3 model. For this, we need to read the access key in from a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6293838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "with open(os.path.expanduser('~/.openai_api_key'), 'r') as file:\n",
    "    OPENAI_API_KEY = file.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f74aec",
   "metadata": {},
   "source": [
    "First, we create the generator object which AdaTest will use to suggest more tests which are similar to the ones we provide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e310578",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = adatest.generators.OpenAI('curie', api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56095496",
   "metadata": {},
   "source": [
    "Now we create the test tree. We will load a set of tests which we have alreadys started work on, to make the process faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc4e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = adatest.TestTree(\"imdb_hotel_conversion.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58006371",
   "metadata": {},
   "source": [
    "And fire up the AdaTest interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056dbba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.adapt(original_pipeline, generator, auto_save=True, recompute_scores=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b61cb927",
   "metadata": {},
   "source": [
    "With a set of samples composed, we need to use them to finetune the model. To begin this process, load the CSV file we've created into a DataFrame and drop the portions we don't need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a790c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_adatest_data(csv_file: str):\n",
    "    tmp = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Drop topic marker rows\n",
    "    tmp2 = tmp[tmp['label'] != 'topic_marker']\n",
    "    # Drop suggestion rows\n",
    "    tmp3 = tmp2[tmp2['topic'] != 'suggestion']\n",
    "    \n",
    "    # Remove columns we don't need\n",
    "    tmp4 = tmp3.drop(labels=['labeler', 'description', 'author', 'Unnamed: 0'], axis=1)\n",
    "    \n",
    "    # Rename columns\n",
    "    tmp5 = tmp4.rename(mapper={'input': 'sentence', 'label': 'model_is_correct'}, axis=1)\n",
    "    \n",
    "    # Don't need to track original rows\n",
    "    tmp6 = tmp5.reset_index(drop=True)\n",
    "    \n",
    "    return tmp6\n",
    "\n",
    "\n",
    "test_data = load_adatest_data('imdb_hotel_conversion.csv')\n",
    "display(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1777931",
   "metadata": {},
   "source": [
    "Next, we need to get the actual labels corresponding to each sentence. For this we need to combine the column which contains the output of our model and the column containing our manual labelling of whether the model was correct or incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e8a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_label(row):\n",
    "    # The model output is either 'pos' or 'neg'\n",
    "    model_result = row['output']\n",
    "    # Return based on whether the model response was marked correct or incorrect\n",
    "    if row['model_is_correct'] == 'pass':\n",
    "        return model_result\n",
    "    else:\n",
    "        if model_result == 'pos':\n",
    "            return 'neg'\n",
    "        else:\n",
    "            return 'pos'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11af53ac",
   "metadata": {},
   "source": [
    "Apply this to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6ec391",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['label'] = test_data.apply(generate_label, axis=1)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4031033a",
   "metadata": {},
   "source": [
    "We can also call the pipeline directly on the sentences we have generated, and make sure that we get the same results as the one stored by AdaTest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649de984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_label(label_probabilities):\n",
    "    # The pipeline returns all of the label probabilities\n",
    "    # We need to extract the largest\n",
    "    max_score = 0\n",
    "    label = None\n",
    "    for l in label_probabilities:\n",
    "        if l['score'] > max_score:\n",
    "            max_score = l['score']\n",
    "            label = l['label']\n",
    "    return label\n",
    "\n",
    "y_pred = [get_label(x) for x in original_pipeline(test_data.sentence.to_list())]\n",
    "\n",
    "\n",
    "test_data['my_y_pred'] = y_pred\n",
    "assert np.array_equal(test_data['my_y_pred'], test_data['output'])\n",
    "\n",
    "display(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385dbdff",
   "metadata": {},
   "source": [
    "We can also evaluate our chosen metric, and check that the accuracy score matches that we expect from the summary at the top level of the AdaTest widget:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2e1f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric_name = 'accuracy'\n",
    "\n",
    "metric = load_metric(metric_name)\n",
    "\n",
    "def label_to_int(l: str) -> int:\n",
    "    # Use the mapping provided by the model\n",
    "    return model.config.label2id[l]\n",
    "\n",
    "metric.compute(predictions=test_data['my_y_pred'].apply(label_to_int), references=test_data['label'].apply(label_to_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8348a9",
   "metadata": {},
   "source": [
    "There is one final tweak to make to our data prior to finetuning the model: the Hugging Face `Trainer`s do not use the human-friendly labels, but the corresponding integer ids. So use the mapping provided by the model to convert the 'label' column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac67a09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['label'] = test_data['label'].apply(label_to_int)\n",
    "print(test_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a4ee2",
   "metadata": {},
   "source": [
    "Now, we can split our dataset into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24150831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bab0273",
   "metadata": {},
   "source": [
    "We stratify based on the 'topic' column, to ensure that we have samples from all of the various topics we have generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d7fbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(test_data, stratify=test_data['topic'], test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7b887f",
   "metadata": {},
   "source": [
    "Convert our DataFrames into Hugging Face `Dataset`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa80d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_ds = Dataset.from_pandas(df = train_df)\n",
    "test_ds = Dataset.from_pandas(df = test_df)\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5197b33",
   "metadata": {},
   "source": [
    "Encode our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffb55b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    result = tokenizer(examples[\"sentence\"],\n",
    "                       add_special_tokens = True,\n",
    "                       truncation = True,\n",
    "                       padding = \"max_length\",\n",
    "                       return_attention_mask = True\n",
    "                      )\n",
    "    return result\n",
    "\n",
    "train_encoded = train_ds.map(preprocess_function, batched=True)\n",
    "test_encoded = test_ds.map(preprocess_function, batched=True)\n",
    "\n",
    "drop_cols = ['topic', '__index_level_0__','model_is_correct', 'model score', 'my_y_pred', 'output']\n",
    "\n",
    "train_encoded = train_encoded.remove_columns(drop_cols)\n",
    "test_encoded = test_encoded.remove_columns(drop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76e81ed",
   "metadata": {},
   "source": [
    "Configure a new training run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26489b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "args_ft = TrainingArguments(\n",
    "    f\"hotel_fine_tuned\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf346f1",
   "metadata": {},
   "source": [
    "Now, load a fresh copy of the model for fine tuning. This will allow us to compare the two models side-by-side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8ddbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = AutoModelForSequenceClassification.from_pretrained(base_model_name,num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca9df6b",
   "metadata": {},
   "source": [
    "Create our new `Trainer` object, using the model we've just loaded. We pass in our new datasets for training and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e8e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Predictions are probabilities, so the actual answer is the index with the highest probability\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "trainer_ft = Trainer(\n",
    "    ft_model,\n",
    "    args_ft,\n",
    "    train_dataset=train_encoded,\n",
    "    eval_dataset=test_encoded,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f104c611",
   "metadata": {},
   "source": [
    "Now, we can run the training. On a CPU, this may take a few minutes (large values of 'few' may be experienced):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a276759",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer_ft.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ebf95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_ft.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58956445",
   "metadata": {},
   "source": [
    "## Assessing the Fine-Tuned Model\n",
    "\n",
    "Now that we have fine-tuned the model with some examples which talk about hotels, we can see if it performs better. First, we put the new model into a scoring pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1d2687",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_pipeline = pipeline(\"sentiment-analysis\",\n",
    "                       model=trainer_ft.model.to('cpu'),\n",
    "                       tokenizer=tokenizer,\n",
    "                       top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39b78c5",
   "metadata": {},
   "source": [
    "We can re-run the initial samples we tried above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7586109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_pipeline(\"Great cinematography but a poor movie overall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1d6211",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_pipeline(\"Snappy dialogue makes for enjoyable entertainment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95509e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_pipeline(\"Located on a busy street with much traffic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53455b62",
   "metadata": {},
   "source": [
    "The sentences about movies are still well classified, but the final one about a hotel has the correct prediction now.\n",
    "\n",
    "For a more systematic comparison, we can run our `test_df` through both pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28332c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(label_probabilities):\n",
    "    # The pipeline returns all of the label probabilities\n",
    "    # We need to extract the largest\n",
    "    max_score = 0\n",
    "    label = None\n",
    "    for l in label_probabilities:\n",
    "        if l['score'] > max_score:\n",
    "            max_score = l['score']\n",
    "            label = l['label']\n",
    "    # Convert back to the id\n",
    "    return ft_model.config.label2id[label]\n",
    "\n",
    "y_pred_orig = [get_label(x) for x in original_pipeline(test_df.sentence.to_list())]\n",
    "y_pred_ft = [get_label(x) for x in ft_pipeline(test_df.sentence.to_list())]\n",
    "\n",
    "print(\"Original  : \", metric.compute(predictions=y_pred_orig, references=test_df.label))\n",
    "print(\"Fine Tuned: \", metric.compute(predictions=y_pred_ft, references=test_df.label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7c536a",
   "metadata": {},
   "source": [
    "We see a noticeable improvement in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01399407",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
